{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_learning.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dchui1/659-project/blob/master/Q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "X9LcrJAnlF8W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "class RiverSwim:\n",
        "    def __init__(self):\n",
        "        self.STEPS_LIMIT = 20000 # number of steps in each episode?\n",
        "        self.pos = 0\n",
        "        self.swimRightStay = 0.6\n",
        "        self.swimRightUp = 0.35\n",
        "        self.swimRightDown = 0.05\n",
        "        self.S1swimRightUp = 0.6\n",
        "        self.SNswimRightDown = 0.4\n",
        "\n",
        "    def reset(self):\n",
        "        self.n = 0\n",
        "        self.pos = 0\n",
        "        return self.pos\n",
        "\n",
        "    def step(self, a): # the transition function?\n",
        "        # if action is 0 then we do nothing\n",
        "        if a == 1:\n",
        "            # determine if we will successfully take the \"up\" action\n",
        "            flip = random.random()\n",
        "            if self.pos <= 0: # first state in chain\n",
        "                if flip > self.S1swimRightUp:\n",
        "                    self.pos = self.pos + 1\n",
        "            elif self.pos >= 5: # end of chain\n",
        "                if flip <= self.SNswimRightDown:\n",
        "                    self.pos = self.pos - 1\n",
        "            else: # middle of chain\n",
        "                if flip <= self.swimRightDown:\n",
        "                    self.pos = self.pos - 1\n",
        "                elif flip > self.swimRightDown + self.swimRightStay:\n",
        "                    self.pos = self.pos + 1\n",
        "        # make sure that the position we return (the next state) is between 0 and 5\n",
        "        self.pos = np.clip(self.pos, 0, 5)\n",
        "\n",
        "        # tuple indicating (state, reward, terminated, action)\n",
        "        # note this is a continuing task, so the environment will never send a\n",
        "        # termination signal\n",
        "        return (self.pos, self.rewardFunction(self.pos), False, a)\n",
        "\n",
        "    def rewardFunction(self, x): # the reward function is deterministic and only depends on state, not action\n",
        "        if x >= 5:\n",
        "            return 1.0\n",
        "        if x <= 0:\n",
        "            return 5.0/1000.0\n",
        "        return 0.0\n",
        "\n",
        "    def numObservations(self):\n",
        "        # position on the river.\n",
        "        # states are: 0, 1, 2, 3, 4, 5\n",
        "        return 1\n",
        "\n",
        "    def numActions(self):\n",
        "        # (0) stay or (1) swim up the river\n",
        "        return 2\n",
        "    # Daniel was here\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P9qEhqZP-lJm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2DpdW_i9-lcY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aVP6RyH9Sub0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Imports and Definitions"
      ]
    },
    {
      "metadata": {
        "id": "EKBU-0t4lHs7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Q:\n",
        "    def __init__(self):\n",
        "        self.alpha = 0.01\n",
        "        self.gamma = 0.99\n",
        "        self.epsilon = 0.0\n",
        "\n",
        "        num_states = 6\n",
        "        self.num_acts = 2\n",
        "\n",
        "        self.Q = np.zeros((num_states, self.num_acts))\n",
        "        self.next_action = 0\n",
        "\n",
        "    def policy(self, S):\n",
        "        if random.random() < self.epsilon:\n",
        "            return random.randint(0, self.num_acts - 1)\n",
        "        return self.maxAction(S)\n",
        "\n",
        "    def maxAction(self, s):\n",
        "        act_vals = self.Q[s, :]\n",
        "        move = self.breakTie(act_vals)\n",
        "        return move\n",
        "\n",
        "    def getAction(self, Obs):\n",
        "        return self.next_action\n",
        "\n",
        "    # if gamma_tp1 = 0, that means the episode terminated\n",
        "    def learn(self, s, sp, r, a, gamma):\n",
        "        ap = self.maxAction(sp)\n",
        "        Q_p = self.Q[sp, ap]\n",
        "\n",
        "        tde = (r + gamma * Q_p) - self.Q[s, a]\n",
        "        self.Q[s, a] = self.Q[s, a] + self.alpha*tde\n",
        "\n",
        "    def update(self, S, Sp, r, a, done):\n",
        "        if done:\n",
        "            self.learn(S, Sp, r, a, 0)\n",
        "        else:\n",
        "            self.next_action = self.policy(Sp)\n",
        "            self.learn(S, Sp, r, a, self.gamma)\n",
        "\n",
        "    def start(self, obs):\n",
        "        self.next_action = self.policy(obs)\n",
        "        return self.next_action\n",
        "\n",
        "    def breakTie(self, act_vals):\n",
        "        indexes = np.where(act_vals == np.max(act_vals))[0]\n",
        "        if len(indexes) < 1:\n",
        "            print(indexes, act_vals)\n",
        "        return np.random.choice(indexes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kfssKq8mR3UP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Q-learning Agent with No Bonus"
      ]
    },
    {
      "metadata": {
        "id": "V0419LB_lOy9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def q_learning(env, num_episodes, q):\n",
        "  for episode in range(num_episodes):\n",
        "    s = env.reset()\n",
        "    a = q.start(s)\n",
        "    \n",
        "    for step in range(env.STEPS_LIMIT):\n",
        "      (sp, r, done, __) = env.step(a) # Note: the environment \"registers\" the new sp as env.pos\n",
        "      done = done or step == (env.STEPS_LIMIT - 1)\n",
        "      q.update(s, sp, r, a, done)\n",
        "      s = sp # update the current state to sp\n",
        "      a = q.policy(s) # update the current action to a\n",
        "  return q.Q   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bAMUk30jbHy1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_episodes = 1\n",
        "env = RiverSwim()\n",
        "Q_object = Q()  # recall: self.Q = np.zeros((num_states, self.num_acts)), thus \"q\" here is an instance of the Q class, where self.Q is an np.array\n",
        "Q_array = q_learning(env, num_episodes, Q_object) \n",
        "Q_array"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ga_3I2i0bJJd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(Q_array.shape)\n",
        "type(Q_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j8vqUmBASJmB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "####Q-learning Agent with Bonus updated Tabularly"
      ]
    },
    {
      "metadata": {
        "id": "hiyJbgK-SRvH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class B:\n",
        "  def __init__(self):\n",
        "    self.B = np.zeros((num_states, self.num_acts, 2))\n",
        "    self.count = 0\n",
        "  \n",
        "  def stat_parameters(self, s, a):\n",
        "    return (self.B[s, a, 0], self.B[s, a, 1])\n",
        "    \n",
        "  def update_stats(self, s, a, val=0.0): # the default of the new value is 0 for exploration bonuses\n",
        "    self.count += 1\n",
        "    old_m, old_var = self.stat_parameters(s, a)\n",
        "    self.B[s, a, 0] = old_m + (val - old_m)/self.count  \n",
        "#     self.B[s, a, 1] = old_var + (val - old_m)*(val - self.B[s, a, 0])\n",
        "    self.B[s, a, 1] = old_var + ((val - old_m)^2)/self.count - (old_var^2)/(n-1)\n",
        "    \n",
        "  def sample(self, s, a):\n",
        "    mean, var = self.stat_parameters(s, a)\n",
        "    return np.random.normal(mean, var)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HHI4oR-Y-pmO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Notes:\n",
        "# windy gridworld -> stochastic world.. maybe ignore stochasticity at first\n",
        "# Try mountain car? This is a continuous-state domain\n",
        "# river swim: states have far enough variance... How is this determined?"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}