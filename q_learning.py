# -*- coding: utf-8 -*-
"""Copy of Q_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKKgtfD69kIQbiw_P5RU-snr8sAEWY4G

####Imports and Definitions
"""
import random
import math
import argparse
import os
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from bayesianapproximator import *
from BNNApproximation import BNNApproximation
from ExperimentDescription import ExperimentDescription
from bayesianapproximator import TDistBayesianApproximation
import utils.registry as registry
from pickle import dump
tf.enable_eager_execution()


def check_algebra(X):
    size = X.shape
    Xt = np.transpose(X)
    XtX = Xt @ X
    XtX_inv = np.linalg.inv(XtX)
    Z = X @ np.transpose(XtX_inv) @ Xt
    # Id = np.eye(size[0])
    min_el = np.min(Z)
    max_el = np.max(Z)
    return (min_el, max_el)


def runExperiment(env, num_episodes, agent, agent_factory):
    total_reward = 0
    rewards = []
    steps = []

    for episode in range(num_episodes):
        s = env.reset()
        a = agent.start(s)
        done = False
        step = 0

        while not done:
            (sp, r, done, __) = env.step(
                a)  # Note: the environment "registers" the new sp as env.pos
            try:
                agent.update(s, sp, r, a, done)
            except ArithmeticError:
                print("Arithmetic exception raised.")
                print("Episode", episode, " Step", step)
                data_mat = np.vstack(agent.data)
                rewards_mat = np.vstack(agent.reward_data)
                print("data_mat.shape and rewards_mat.shape:")
                print(data_mat.shape)
                print(rewards_mat.shape)

                min_element = np.min(data_mat)
                max_element = np.max(data_mat)
                print("min and max element of data_mat:")
                print(min_element)
                print(max_element)
                # min_Z, max_Z = check_algebra(data_mat)
                # print("")
                # print(min_Z)
                # print(max_Z)
                new_agent = agent_factory()

                new_agent.rewardApprox.update_stats(data_mat[:-1],
                                                    rewards_mat[:-1])
                print("parameters of normal and ig disributions:")
                normal_covariance = new_agent.rewardApprox.T_distribution.mnig_prior.normal_prior.covariance_scale
                normal_precision = new_agent.rewardApprox.T_distribution.mnig_prior.normal_prior.precision
                ig_scale = new_agent.rewardApprox.T_distribution.mnig_prior.ig_prior.scale
                ig_shape = new_agent.rewardApprox.T_distribution.mnig_prior.ig_prior.shape

                print(normal_covariance)
                print(normal_precision)
                print(ig_scale)
                print(ig_shape)

                data_dict = {
                    'normal_covariance': normal_covariance,
                    'normal_precision': normal_precision,
                    'ig_scale': ig_scale,
                    'ig_shape': ig_shape,
                    'X': data_mat,
                    'y': rewards_mat
                }

                np.save("tmp/debugging", data_dict)
                exit()

            s = sp  # update the current state to sp
            # ac_vals = agent.action_values(s)
            # print(ac_vals)
            a = agent.getAction(s)  # update the current action to a
            # print("State action pair", s, a)
            total_reward += r
            rewards.append(total_reward)
            step += 1

            # for a in range(env.numActions()):
            #     plt.imshow(agent.rewardApprox.action_var[a].reshape((30, 30)), cmap='hot', vmin=0.0, vmax=0.1)
            #     plt.savefig(f'figs/heat_map.{ss)tep}.{a}.png')

        steps.append(step)
        # print("Episode", episode, " Step", step)
        # agent.print()

    return (steps, rewards)


def averageOverRuns(Agent, Env, exp):
    rewards = []
    total_steps = []
    for run in range(exp.runs):
        env = Env(exp.env_params)
        np.random.seed(run)
        tf.random.set_random_seed(run)
        random.seed(run)
        agent = Agent(env.observationShape(), env.numActions(),
                      exp.meta_parameters)

        def agent_factory():
            return Agent(env.observationShape(), env.numActions(),
                         exp.meta_parameters)

        (steps, r) = runExperiment(env, exp.env_params['episodes'], agent,
                                   agent_factory)
        rewards.append(r)
        print("Completed a run")
        total_steps.append(steps)
        # print("Completed run %d of %d"%(, exp.runs)

    metric = np.array(total_steps)
    mean = metric.mean(axis=0)
    stderr = metric.std(axis=0) / np.sqrt(exp.runs)

    return (mean, stderr)


def confidenceInterval(mean, stderr):
    return (mean - stderr, mean + stderr)


def plotRewards(ax, rewards, stderr, label):
    (low_ci, high_ci) = confidenceInterval(rewards, stderr)
    ax.plot(rewards, label=label)
    ax.fill_between(range(rewards.shape[0]), low_ci, high_ci, alpha=0.4)


def parse_args():
    parser = argparse.ArgumentParser(
        "Reinforcement Learning experiments for multiagent environments")
    parser.add_argument(
        "-i", type=int, help="integer choosing parameter permutation to run")
    parser.add_argument(
        "-e", type=str, help="path to experiment description json file")
    parser.add_argument("-r", type=int, help="number of runs to complete")
    parser.add_argument(
        "-b", type=str, default='results', help="base path for saving results")

    args = parser.parse_args()
    if args.b == None or args.r == None or args.i == None:
        print('Please run again using (without angle braces):')
        print('python q_learning.py -e path/to/exp.json -i <num> -r <num>')
        exit(1)

    return args


args = parse_args()
exp = ExperimentDescription(args.e, args.i, args.r)

Env = registry.getEnvironment(exp)
Agent = registry.getAgent(exp)

print("here")
(rewards, stderr) = averageOverRuns(Agent, Env, exp)

fig = plt.figure()
ax = plt.axes()
plotRewards(ax, rewards, stderr, "LinearQ_TDistR")

# save some metric for performance to file
meanResult = np.mean(rewards)
path = f'{args.b}/{exp.name}/{exp.environment}/{exp.agent}/{exp.getParamString()}'
os.makedirs(path, exist_ok=True)
with open(f'{path}/mean.csv', 'w') as f:
    f.write(str(meanResult))

with open(f'{path}/results.pkl', 'wb') as f:
    dump({"results": (rewards, stderr)}, f)

plt.legend()
plt.title("Average Number of Steps to Reach Goal across 1 Runs")
plt.xlabel("Number of Episodes")
plt.ylabel("Average Number of Steps to Reach Goal")
plt.show()
