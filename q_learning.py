# -*- coding: utf-8 -*-
"""Copy of Q_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKKgtfD69kIQbiw_P5RU-snr8sAEWY4G

####Imports and Definitions
"""
import random
import math
import numpy as np
import matplotlib.pyplot as plt
import gym
import gym_gridworlds



class Optimal:
  def __init__(self, num_states, num_acts):
    self.num_states = num_states
    self.num_acts = num_acts

  def policy(self, s):
    return 1
  def getAction(self, s):
    return 1

  def update(self, s, sp, r, a, done):
    return None

  def start(self, s):
    return 1

class Q:
  def __init__(self, num_states, num_acts):
    self.alpha = 0.01
    self.gamma = 0.99
    self.epsilon = 0.1

    self.num_states = num_states
    self.num_acts = num_acts

    self.Q = np.zeros((self.num_states, self.num_acts))
    self.next_action = 0

  def policy(self, S):
    if random.random() < self.epsilon:
      return random.randint(0, self.num_acts - 1)
    return self.maxAction(S)

  def maxAction(self, s):
    act_vals = self.Q[s, :]
    move = self.breakTie(act_vals)
    return move

  def getAction(self, Obs):
    return self.next_action

  # if gamma_tp1 = 0, that means the episode terminated
  def learn(self, s, sp, r, a, gamma, max_bonus=0):
    ap = self.maxAction(sp)
    Q_p = self.Q[sp, ap]

    tde = (r + max_bonus + gamma * Q_p) - self.Q[s, a]  # add a max_bonus i
    self.Q[s, a] = self.Q[s, a] + self.alpha*tde

  def update(self, S, Sp, r, a, done, max_bonus=0):
    if done:
      self.learn(S, Sp, r, a, 0) # If done, whould we give an exploration bonus?
    else:
      self.next_action = self.policy(Sp)
      self.learn(S, Sp, r, a, self.gamma, max_bonus)

  def start(self, obs):
    self.next_action = self.policy(obs)
    return self.next_action

  def breakTie(self, act_vals):
    indexes = np.where(act_vals == np.max(act_vals))[0]
    print("Indexes in break tie", indexes)
    if len(indexes) < 1:
      print(indexes, act_vals)
      return 0
    return np.random.choice(indexes)

"""####Q-learning Agent with No Bonus"""

class TabularBayesianApproximation:
  def __init__(self, num_states, num_acts):
    self.B = np.zeros((num_states, num_acts, 4))
    # prior sample mean
    # prior "observations to make that mean"
    # prior "observations to make our variance" # try to make it hard to reduce this
    # prior sum of square errors (proportional to initial sample variance)
    self.B[:, :] = [1, 1, 1, 4]

  def update_stats(self, s, a, val=0.0): # the default of the new value is 0 for exploration bonuses
    print("THe s and a", s, a)
    index = s[0] * 10 + s[1]
    print("Index", index)
    mu, nu, alpha, beta = self.B[index, a, :]
    self.B[s, a, 0] = (nu * mu + val) / (nu + 1)
    self.B[s, a, 1] = nu + 1
    self.B[s, a, 2] = alpha + 1.0/2.0
    self.B[s, a, 3] = (nu / (nu + 1.0)) * math.pow((val - mu), 2.0) / 2.0

  def sample(self, s, a, n):
    index = s[0] * 10 + s[1]
    mu, nu, alpha, beta = self.B[index, a, :]
    variance = beta / ((alpha - 1.0) * nu)
    # don't add the mean here so we do not double count for the reward
    one_stdev = np.sqrt(variance)
    return [ one_stdev ]

"""####Q-learning Agent with Bonus updated Tabularly"""

# class QEBValueFunction(Q):
#   def __init__(self, num_states, num_acts):
#     super().__init__(num_states, num_acts)
#     self.B = TabularBayesianApproximation(num_states, num_acts)

#   def update(self, s, sp, r, a, done):
#     self.B.update_stats(s, a, 0)
#     bonus = max(self.B.sample(s, a, 10))
#     super().update(s, sp, r + bonus, a, done)

class QRewardValueFunction(Q):
  def __init__(self, num_states, num_acts):
    super().__init__(num_states, num_acts)
    self.B = TabularBayesianApproximation(num_states, num_acts)
    print("Size of b", type(self.B))
    self.epsilon = 0.01

  def update(self, s, sp, r, a, done):

    self.B.update_stats(s, a, r)
    bonus = max(self.B.sample(s, a, 10))
    super().update(s, sp, r + bonus, a, done)

def runExperiment(env, num_episodes, q):
  total_reward = 0
  rewards = []
  for episode in range(num_episodes):
    s = env.reset()
    a = q.start(s)
    limit = 10000
    for step in range(limit):
      (sp, r, done, __) = env.step(a) # Note: the environment "registers" the new sp as env.pos
      done = done or step == (limit - 1)
      q.update(s, sp, r, a, done)
      s = sp # update the current state to sp
      a = q.getAction(s) # update the current action to a

      total_reward += r
      rewards.append(total_reward)

  return rewards


def averageOverRuns(Agent, env, runs = 20):
  rewards = []
  for run in range(runs):
    np.random.seed(run)
    random.seed(run)

    agent = Agent(70, 4)
    # env = Env()

    r = runExperiment(env, 1, agent)
    rewards.append(r)

  np_rewards = np.array(rewards)
  mean = np_rewards.mean(axis=0)
  stderr = np_rewards.std(axis=0) / np.sqrt(runs)

  return (mean, stderr)

def confidenceInterval(mean, stderr):
  return (mean - stderr, mean + stderr)

def plotRewards(ax, rewards, stderr, label):
  (low_ci, high_ci) = confidenceInterval(rewards, stderr)
  ax.plot(rewards, label=label)
  ax.fill_between(range(rewards.shape[0]), low_ci, high_ci, alpha=0.4)

fig = plt.figure()
ax = plt.axes()

env = gym.make('WindyGridworld-v0')
(rewards, stderr) = averageOverRuns(Optimal, env, 20)
plotRewards(ax, rewards, stderr, 'Optimal')


# (rewards, stderr) = averageOverRuns(QRewardActionSelection, RiverSwim, 20)
# plotRewards(ax, rewards, stderr, 'QReward action-selection')

(rewards, stderr) = averageOverRuns(Q, env, 20)
plotRewards(ax, rewards, stderr, 'Q epsilon=0.1')

(rewards, stderr) = averageOverRuns(QRewardValueFunction, env, 20)
plotRewards(ax, rewards, stderr, 'QReward value-function')

plt.legend()
plt.show()

# Notes:
# windy gridworld -> stochastic world.. maybe ignore stochasticity at first
# Try mountain car? This is a continuous-state domain
# river swim: states have far enough variance... How is this determined?
