# -*- coding: utf-8 -*-
"""Copy of Q_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKKgtfD69kIQbiw_P5RU-snr8sAEWY4G

####Imports and Definitions
"""
import random
import math
import numpy as np
import matplotlib.pyplot as plt

class RiverSwim:
  def __init__(self):
    self.STEPS_LIMIT = 10000 # number of steps in each episode?
    self.pos = 0
    self.swimRightStay = 0.6
    self.swimRightUp = 0.35
    self.swimRightDown = 0.05
    self.S1swimRightUp = 0.6
    self.SNswimRightDown = 0.4

  def reset(self):
      self.n = 0
      self.pos = 0
      return self.pos

  def step(self, a): # the transition function?
    old_pos = self.pos
    # if action is 0 then we do nothing
    if a == 1:
      # determine if we will successfully take the "up" action
      flip = random.random()
      if self.pos <= 0: # first state in chain
        if flip > self.S1swimRightUp:
          self.pos = self.pos + 1
      elif self.pos >= 5: # end of chain
        if flip <= self.SNswimRightDown:
          self.pos = self.pos - 1
      else: # middle of chain
        if flip <= self.swimRightDown:
          self.pos = self.pos - 1
        elif flip > self.swimRightDown + self.swimRightStay:
          self.pos = self.pos + 1
    # make sure that the position we return (the next state) is between 0 and 5
    self.pos = np.clip(self.pos, 0, 5)

    # tuple indicating (state, reward, terminated, action)
    # note this is a continuing task, so the environment will never send a
    # termination signal
    return (self.pos, self.rewardFunction(old_pos, a), False, a)

  def rewardFunction(self, x, a):
    if x >= 5 and a == 1:
      return 1.0
    if x <= 0 and a == 0:
      return 5.0/1000.0
    return 0.0

  def numObservations(self):
    # position on the river.
    # states are: 0, 1, 2, 3, 4, 5
    return 1

  def numActions(self):
    # (0) stay or (1) swim up the river
    return 2

class Optimal:
  def __init__(self, num_states, num_acts):
    self.num_states = num_states
    self.num_acts = num_acts

  def policy(self, s):
    return 1
  def getAction(self, s):
    return 1

  def update(self, s, sp, r, a, done):
    return None

  def start(self, s):
    return 1

class Q:
  def __init__(self, num_states, num_acts):
    self.alpha = 0.01
    self.gamma = 0.99
    self.epsilon = 0.1

    self.num_states = num_states
    self.num_acts = num_acts

    self.Q = np.zeros((self.num_states, self.num_acts))
    self.next_action = 0

  def policy(self, S):
    if random.random() < self.epsilon:
      return random.randint(0, self.num_acts - 1)
    return self.maxAction(S)

  def maxAction(self, s):
    act_vals = self.Q[s, :]
    move = self.breakTie(act_vals)
    return move

  def getAction(self, Obs):
    return self.next_action

  # if gamma_tp1 = 0, that means the episode terminated
  def learn(self, s, sp, r, a, gamma, max_bonus=0):
    ap = self.maxAction(sp)
    Q_p = self.Q[sp, ap]

    tde = (r + max_bonus + gamma * Q_p) - self.Q[s, a]  # add a max_bonus i
    self.Q[s, a] = self.Q[s, a] + self.alpha*tde

  def update(self, S, Sp, r, a, done, max_bonus=0):
    if done:
      self.learn(S, Sp, r, a, 0) # If done, whould we give an exploration bonus?
    else:
      self.next_action = self.policy(Sp)
      self.learn(S, Sp, r, a, self.gamma, max_bonus)

  def start(self, obs):
    self.next_action = self.policy(obs)
    return self.next_action

  def breakTie(self, act_vals):
    indexes = np.where(act_vals == np.max(act_vals))[0]
    if len(indexes) < 1:
      print(indexes, act_vals)
    return np.random.choice(indexes)

"""####Q-learning Agent with No Bonus"""

class TabularBayesianApproximation:
  def __init__(self, num_states, num_acts):
    self.B = np.zeros((num_states, num_acts, 4))
    # prior sample mean
    # prior "observations to make that mean"
    # prior "observations to make our variance" # try to make it hard to reduce this
    # prior sum of square errors (proportional to initial sample variance)
    self.B[:, :] = [1, 1, 1, 4]

  def update_stats(self, s, a, val=0.0): # the default of the new value is 0 for exploration bonuses
    mu, nu, alpha, beta = self.B[s, a, :]
    self.B[s, a, 0] = (nu * mu + val) / (nu + 1)
    self.B[s, a, 1] = nu + 1
    self.B[s, a, 2] = alpha + 1.0/2.0
    self.B[s, a, 3] = (nu / (nu + 1.0)) * math.pow((val - mu), 2.0) / 2.0

  def sample(self, s, a, n):
    mu, nu, alpha, beta = self.B[s, a, :]
    variance = beta / ((alpha - 1.0) * nu)
    # don't add the mean here so we do not double count for the reward
    one_stdev = np.sqrt(variance)
    return [ one_stdev ]

"""####Q-learning Agent with Bonus updated Tabularly"""

# class QEBValueFunction(Q):
#   def __init__(self, num_states, num_acts):
#     super().__init__(num_states, num_acts)
#     self.B = TabularBayesianApproximation(num_states, num_acts)

#   def update(self, s, sp, r, a, done):
#     self.B.update_stats(s, a, 0)
#     bonus = max(self.B.sample(s, a, 10))
#     super().update(s, sp, r + bonus, a, done)

class QRewardValueFunction(Q):
  def __init__(self, num_states, num_acts):
    super().__init__(num_states, num_acts)
    self.B = TabularBayesianApproximation(num_states, num_acts)
    self.epsilon = 0.01

  def update(self, s, sp, r, a, done):
    self.B.update_stats(s, a, r)
    bonus = max(self.B.sample(s, a, 10))
    super().update(s, sp, r + bonus, a, done)

def runExperiment(env, num_episodes, q):
  total_reward = 0
  rewards = []
  for episode in range(num_episodes):
    s = env.reset()
    a = q.start(s)

    for step in range(env.STEPS_LIMIT):
      (sp, r, done, __) = env.step(a) # Note: the environment "registers" the new sp as env.pos
      done = done or step == (env.STEPS_LIMIT - 1)
      q.update(s, sp, r, a, done)
      s = sp # update the current state to sp
      a = q.getAction(s) # update the current action to a

      total_reward += r
      rewards.append(total_reward)

  return rewards


def averageOverRuns(Agent, Env, runs = 20):
  rewards = []
  for run in range(runs):
    np.random.seed(run)
    random.seed(run)

    agent = Agent(6, 2)
    env = Env()

    r = runExperiment(env, 1, agent)
    rewards.append(r)

  np_rewards = np.array(rewards)
  mean = np_rewards.mean(axis=0)
  stderr = np_rewards.std(axis=0) / np.sqrt(runs)

  return (mean, stderr)

def confidenceInterval(mean, stderr):
  return (mean - stderr, mean + stderr)

def plotRewards(ax, rewards, stderr, label):
  (low_ci, high_ci) = confidenceInterval(rewards, stderr)
  ax.plot(rewards, label=label)
  ax.fill_between(range(rewards.shape[0]), low_ci, high_ci, alpha=0.4)

fig = plt.figure()
ax = plt.axes()


(rewards, stderr) = averageOverRuns(Optimal, RiverSwim, 20)
plotRewards(ax, rewards, stderr, 'Optimal')

# (rewards, stderr) = averageOverRuns(QEBActionSelection, RiverSwim, 20)
# plotRewards(ax, rewards, stderr, 'QEB action-selection')

# (rewards, stderr) = averageOverRuns(QRewardActionSelection, RiverSwim, 20)
# plotRewards(ax, rewards, stderr, 'QReward action-selection')

(rewards, stderr) = averageOverRuns(Q, RiverSwim, 20)
plotRewards(ax, rewards, stderr, 'Q epsilon=0.1')

# (rewards, stderr) = averageOverRuns(QEBValueFunction, RiverSwim, 20)
# plotRewards(ax, rewards, stderr, 'QEB value-function')

(rewards, stderr) = averageOverRuns(QRewardValueFunction, RiverSwim, 20)
plotRewards(ax, rewards, stderr, 'QReward value-function')

plt.legend()
plt.show()

# Notes:
# windy gridworld -> stochastic world.. maybe ignore stochasticity at first
# Try mountain car? This is a continuous-state domain
# river swim: states have far enough variance... How is this determined?
