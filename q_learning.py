# -*- coding: utf-8 -*-
"""Copy of Q_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKKgtfD69kIQbiw_P5RU-snr8sAEWY4G

####Imports and Definitions
"""
from functools import reduce
import random
import math
import numpy as np
import matplotlib.pyplot as plt
from environments.gridworld import GridWorld

product = lambda arr: reduce(lambda a, b: a * b, arr)

class Optimal:
  def __init__(self, num_states, num_acts):
    self.num_states = num_states
    self.num_acts = num_acts

  def policy(self, s):
    return 1
  def getAction(self, s):
    return 1

  def update(self, s, sp, r, a, done):
    return None

  def start(self, s):
    return 1

class Q:
  def __init__(self, state_shape, num_acts):
    self.alpha = 0.5
    self.gamma = 0.9
    self.epsilon = 0.1

    self.state_shape = state_shape
    self.num_states = product(state_shape)
    self.num_acts = num_acts

    self.Q = np.zeros((self.num_states, self.num_acts))
    self.next_action = 0

  def getIndex(self, s):
    return np.ravel_multi_index(s, self.state_shape)

  def policy(self, S):
    if random.random() < self.epsilon:
      return random.randint(0, self.num_acts - 1)
    return self.maxAction(S)

  def maxAction(self, s):
    act_vals = self.Q[self.getIndex(s), :]
    move = self.breakTie(act_vals)
    return move

  def getAction(self, Obs):
    return self.next_action

  # if gamma_tp1 = 0, that means the episode terminated
  def learn(self, s, sp, r, a, gamma, max_bonus=0):
    ap = self.maxAction(sp)
    Q_p = self.Q[self.getIndex(sp), ap]

    s_idx = self.getIndex(s)

    tde = (r + max_bonus + gamma * Q_p) - self.Q[s_idx, a]  # add a max_bonus i
    self.Q[s_idx, a] = self.Q[s_idx, a] + self.alpha*tde

  def update(self, S, Sp, r, a, done, max_bonus=0):
    if done:
      self.learn(S, Sp, r, a, 0) # If done, whould we give an exploration bonus?
    else:
      self.next_action = self.policy(Sp)
      self.learn(S, Sp, r, a, self.gamma, max_bonus)

  def start(self, obs):
    self.next_action = self.policy(obs)
    return self.next_action

  def breakTie(self, act_vals):
    indexes = np.where(act_vals == np.max(act_vals))[0]
    if len(indexes) < 1:
      raise ArithmeticError()

    return np.random.choice(indexes)

"""####Q-learning Agent with No Bonus"""

class TabularBayesianApproximation:
  def __init__(self, state_shape, num_acts):
    self.num_states = product(state_shape)
    self.state_shape = state_shape
    self.B = np.zeros((self.num_states, num_acts, 4))
    # prior sample mean
    # prior "observations to make that mean"
    # prior "observations to make our variance" # try to make it hard to reduce this
    # prior sum of square errors (proportional to initial sample variance)
    self.B[:, :] = [1, 1, 2, 1]

  def getIndex(self, s):
    return np.ravel_multi_index(s, self.state_shape)

  def update_stats(self, s, a, val=0.0): # the default of the new value is 0 for exploration bonuses
    s_idx = self.getIndex(s)
    mu, nu, alpha, beta = self.B[s_idx, a, :]
    self.B[s_idx, a, 0] = (nu * mu + val) / (nu + 1)
    self.B[s_idx, a, 1] = nu + 1
    self.B[s_idx, a, 2] = alpha + 1.0/2.0
    self.B[s_idx, a, 3] = (nu / (nu + 1.0)) * math.pow((val - mu), 2.0) / 2.0

  def sample(self, s, a, n):
    s_idx = self.getIndex(s)
    mu, nu, alpha, beta = self.B[s_idx, a, :]
    variance = beta / ((alpha - 1.0) * nu)
    # don't add the mean here so we do not double count for the reward
    one_stdev = np.sqrt(variance)
    return [ one_stdev ]

"""####Q-learning Agent with Bonus updated Tabularly"""

class QRewardValueFunction(Q):
  def __init__(self, num_states, num_acts):
    super().__init__(num_states, num_acts)
    self.B = TabularBayesianApproximation(num_states, num_acts)
    self.epsilon = 0.05

  def update(self, s, sp, r, a, done):
    self.B.update_stats(s, a, r)
    bonus = max(self.B.sample(s, a, 10))
    super().update(s, sp, r + bonus, a, done)

def runExperiment(env, num_episodes, q):
  total_reward = 0
  rewards = []
  steps = []

  for episode in range(num_episodes):
    s = env.reset()
    a = q.start(s)
    done = False

    step = 0

    while not done:
      (sp, r, done, __) = env.step(a) # Note: the environment "registers" the new sp as env.pos
      q.update(s, sp, r, a, done)

      s = sp # update the current state to sp
      a = q.getAction(s) # update the current action to a

      total_reward += r
      rewards.append(total_reward)

      step += 1

    steps.append(step)
    print(episode, step)

  return (steps, rewards)


def averageOverRuns(Agent, env, runs = 20):
  rewards = []
  total_steps = []
  for run in range(runs):
    np.random.seed(run)
    random.seed(run)

    agent = Agent(env.observationShape(), env.numActions())

    (steps, r) = runExperiment(env, 1000, agent)
    rewards.append(r)
    total_steps.append(steps)

  # mean_value = np.mean(agent.Q, 1).reshape(30, 30)
  # plt.imshow(mean_value, cmap='hot', interpolation='nearest')
  # plt.gca().invert_yaxis()
  # plt.show()

  metric = np.array(total_steps)
  mean = metric.mean(axis=0)
  stderr = metric.std(axis=0) / np.sqrt(runs)

  return (mean, stderr)

def confidenceInterval(mean, stderr):
  return (mean - stderr, mean + stderr)

def plotRewards(ax, rewards, stderr, label):
  (low_ci, high_ci) = confidenceInterval(rewards, stderr)
  ax.plot(rewards, label=label)
  ax.fill_between(range(rewards.shape[0]), low_ci, high_ci, alpha=0.4)

fig = plt.figure()
ax = plt.axes()

env = GridWorld([30, 30], 400)

# Optimal for riverswim, doesn't make sense on gridworld
# (rewards, stderr) = averageOverRuns(Optimal, env, 20)
# plotRewards(ax, rewards, stderr, 'Optimal')

# (rewards, stderr) = averageOverRuns(Q, env, 1)
# plotRewards(ax, rewards, stderr, 'Q epsilon=0.1')

(rewards, stderr) = averageOverRuns(QRewardValueFunction, env, 5)
plotRewards(ax, rewards, stderr, 'QReward value-function')

plt.legend()
plt.show()

# Notes:
# windy gridworld -> stochastic world.. maybe ignore stochasticity at first
# Try mountain car? This is a continuous-state domain
# river swim: states have far enough variance... How is this determined?
