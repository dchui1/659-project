# -*- coding: utf-8 -*-
"""Copy of Q_learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rKKgtfD69kIQbiw_P5RU-snr8sAEWY4G

####Imports and Definitions
"""
import random
import math
import argparse
import numpy as np
import matplotlib.pyplot as plt
from bayesianapproximator import *
from BNNApproximation import BNNApproximation

# agents
from agents.TabularQ import TabularQ
from agents.TabularRTabularQ import TabularRTabularQ
from agents.BnnRTabularQ import BnnRTabularQ
from agents.RiverswimOptimal import Optimal
from agents.UCB import UCB

# environments
from environments.gridworld import GridWorld

def runExperiment(env, num_episodes, q):
  total_reward = 0
  rewards = []
  steps = []

  for episode in range(num_episodes):
    s = env.reset()
    a = q.start(s)
    done = False

    step = 0

    while not done:
      (sp, r, done, __) = env.step(a) # Note: the environment "registers" the new sp as env.pos
      q.update(s, sp, r, a, done)

      s = sp # update the current state to sp
      a = q.getAction(s) # update the current action to a

      total_reward += r
      rewards.append(total_reward)

      step += 1

    steps.append(step)
    print(episode, step)

  return (steps, rewards)


def averageOverRuns(Agent, env, runs = 20):
  rewards = []
  total_steps = []
  for run in range(runs):
    np.random.seed(run)
    random.seed(run)
    agent = Agent(env.observationShape(), env.numActions())
    (steps, r) = runExperiment(env, 1000, agent)
    rewards.append(r)
    total_steps.append(steps)

  metric = np.array(total_steps)
  mean = metric.mean(axis=0)
  stderr = metric.std(axis=0) / np.sqrt(runs)

  return (mean, stderr)

def confidenceInterval(mean, stderr):
  return (mean - stderr, mean + stderr)

def plotRewards(ax, rewards, stderr, label):
  (low_ci, high_ci) = confidenceInterval(rewards, stderr)
  ax.plot(rewards, label=label)
  ax.fill_between(range(rewards.shape[0]), low_ci, high_ci, alpha=0.4)

fig = plt.figure()
ax = plt.axes()

# def main():
env = GridWorld([30, 30], 700)

# Optimal for riverswim, doesn't make sense on gridworld
# (rewards, stderr) = averageOverRuns(Optimal, env, 20)
# plotRewards(ax, rewards, stderr, 'Optimal')
#
# (rewards, stderr) = averageOverRuns(Q, env, 1)
# plotRewards(ax, rewards, stderr, 'Q epsilon=0.1')

# (rewards, stderr) = averageOverRuns(Q, env, 1)
# plotRewards(ax, rewards, stderr, 'Q epsilon=0.1')

(rewards, stderr) = averageOverRuns(UCB, env, 5)
plotRewards(ax, rewards, stderr, 'UCB')

(rewards, stderr) = averageOverRuns(TabularRTabularQ, env, 5)
plotRewards(ax, rewards, stderr, 'QReward value-function')

plt.legend()
plt.show()



def parse_args():
    parser = argparse.ArgumentParser("Reinforcement Learning experiments for multiagent environments")
    # Environment
    parser.add_argument("--environment", type=str, default="gridworld", help="environment")
    parser.add_argument("--agent", type=str, default="Q", help="environment")
    


    return parser.parse_args()

# Notes:
# windy gridworld -> stochastic world.. maybe ignore stochasticity at first
# Try mountain car? This is a continuous-state domain
# river swim: states have far enough variance... How is this determined?
